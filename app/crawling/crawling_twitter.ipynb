{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import subprocess\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta \n",
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mongo(filepath):\n",
    "    try:\n",
    "        # Koneksi ke MongoDB\n",
    "        with MongoClient('mongodb://localhost:27017/') as client:\n",
    "            db = client.db_analisis_sentimen\n",
    "            tweets_collection = db.tweets\n",
    "\n",
    "            # Membuka file CSV\n",
    "            with open(filepath, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "\n",
    "                for tweet in reader:\n",
    "                    # Pastikan \"id_str\" ada dan tidak kosong\n",
    "                    if tweet.get(\"id_str\") and tweet[\"id_str\"] != \"id_str\":\n",
    "                        existing_tweet = tweets_collection.find_one({\"full_text\": tweet.get(\"full_text\")})\n",
    "\n",
    "                        if not existing_tweet:\n",
    "                            tweet_to_save = {\n",
    "                                \"conversation_id_str\": tweet.get(\"conversation_id_str\"), \n",
    "                                \"created_at\": tweet.get(\"created_at\"), \n",
    "                                \"favorite_count\": tweet.get(\"favorite_count\"),\n",
    "                                \"full_text\": tweet.get(\"full_text\"),\n",
    "                                \"id_str\": tweet.get(\"id_str\"),  \n",
    "                                \"username\": tweet.get(\"username\"),\n",
    "                                \"tweet_url\": tweet.get(\"tweet_url\"),\n",
    "                                \"image_url\": tweet.get(\"image_url\"),  \n",
    "                                \"in_reply_to_screen_name\": tweet.get(\"in_reply_to_screen_name\"),  \n",
    "                                \"lang\": tweet.get(\"lang\"), \n",
    "                                \"location\": tweet.get(\"location\"),  \n",
    "                                \"quote_count\": tweet.get(\"quote_count\"),  \n",
    "                                \"reply_count\": tweet.get(\"reply_count\"),  \n",
    "                                \"retweet_count\": tweet.get(\"retweet_count\"),  \n",
    "                                \"user_id_str\": tweet.get(\"user_id_str\"),  \n",
    "                            }\n",
    "                            tweets_collection.insert_one(tweet_to_save)\n",
    "\n",
    "        print(\"Data berhasil disimpan ke MongoDB.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().date()\n",
    "yesterday = current_date - timedelta(days=1)\n",
    "\n",
    "date_format = yesterday.strftime(\"%Y-%m-%d\")\n",
    "filename = 'tweets.csv'\n",
    "limit = 50\n",
    "\n",
    "search_keyword = f'unnes since:{date_format} until:{current_date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx_command = f'npx --yes tweet-harvest@latest -o \"{filename}\" -s \"{search_keyword}\" -l {limit} --token \"4542b47c1a5e8af47d4968ec7a1d1f6360c3c091\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess Output: \u001b[1m\u001b[32mTweet Harvest [v2.6.1]\u001b[39m\u001b[22m\n",
      "\u001b[1m\u001b[32m\u001b[39m\u001b[22m\n",
      "\u001b[34mResearch by \u001b[39m\u001b[1m\u001b[34mHelmi Satria\u001b[39m\u001b[22m\u001b[34m\u001b[39m\n",
      "\u001b[34mUse it for Educational Purposes only!\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[33mThis script uses Chromium Browser to crawl data from Twitter with \u001b[1myour Twitter auth token\u001b[22m.\u001b[39m\n",
      "\u001b[33mPlease enter your Twitter auth token when prompted.\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m Keep your access token secret! Don't share it with anyone else.\n",
      "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m This script only runs on your local device.\n",
      "\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mOpening twitter search page...\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[33mFilling in keywords: unnes since:2025-03-13 until:2025-03-14\u001b[39m\n",
      "\u001b[33m\u001b[39m\n",
      "\u001b[90m\u001b[39m\n",
      "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[32mCreated new directory: \\\\?\\c:\\Users\\Ahmad Dani\\codingz\\SENTIS\\app\\crawling\\tweets-data\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: c:\\Users\\Ahmad Dani\\codingz\\SENTIS\\app\\crawling\\tweets-data\\tweets.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 19\u001b[39m\n",
      "\u001b[90m\u001b[39m\n",
      "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: c:\\Users\\Ahmad Dani\\codingz\\SENTIS\\app\\crawling\\tweets-data\\tweets.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 39\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34m\u001b[39m\n",
      "\u001b[34mYour tweets saved to: c:\\Users\\Ahmad Dani\\codingz\\SENTIS\\app\\crawling\\tweets-data\\tweets.csv\u001b[39m\n",
      "\u001b[33mTotal tweets saved: 59\u001b[39m\n",
      "Got 59 tweets, done scrolling...\n",
      "\n",
      "Subprocess Errors: \n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run(npx_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding=\"utf-8\")\n",
    "print(\"Subprocess Output:\", result.stdout)\n",
    "print(\"Subprocess Errors:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data berhasil disimpan ke MongoDB.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join('tweets-data', 'tweets.csv')\n",
    "save_mongo(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
